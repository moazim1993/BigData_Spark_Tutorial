{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Tweets from Text files\n",
    "\n",
    "Group Project for Big Data Programming, Fall 2017\n",
    "\n",
    "### Project Contributors:\n",
    "Caleb Hulburt   \n",
    "Mohammad Azim   \n",
    "Yao Jin   \n",
    "Xian Lai   \n",
    "\n",
    "========================================================\n",
    "\n",
    "This application focuses on analysing the tweets in a distributed manner under the framework of Spark. The tweets are coming from 2 files \"MAGA.txt\" and \"resist.txt\". Each file contains the tweets pulled from tweepy API corresponding to label #resist or #maga. \n",
    "\n",
    "Our target is mixing them together and extract the most informative features using term frequency and perform naive The main steps are:\n",
    "1. preprocess each tweet into a clean list of words with stop word removed.\n",
    "2. Mix the 2 tweets datasets together, count the frequencies of each word and fetch the top 5000 words.\n",
    "3. Use these 5000 words as the features for mixed dataset so that each data point has 5000 features and each feature value is either true or false indicating whether this word appears in current tweet.\n",
    "4. Calculate the \"informativeness\" of features.\n",
    "\n",
    "The benefits using Spark for this application is that each data point namely each tweet can be seen independent of other tweets and most of the steps in preprocessing and analysing are independent of other steps. So parallelize them will significant shorten the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from math import log\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we set up spark configuration using **local mode with 2 CPU's** and create a spark context with this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"tweeter_spark\").setMaster(\"local[2]\")\n",
    "sc   = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should prepare the vocabulary of stop words for later filtering. Here the vocabulary is stored as a list in the STOPWORD variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/stop_words.txt\", 'r') as f:\n",
    "    STOPWORDS = f.read()\n",
    "    \n",
    "STOPWORDS = STOPWORDS.split(\"\\n\")\n",
    "STOPWORDS = [word for word in STOPWORDS if word != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. preprocessing\n",
    "We wrap the preprocessing of tweets into function **prepTweets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of resist tweets: 11737\n",
      "Number of maga tweets: 10849\n"
     ]
    }
   ],
   "source": [
    "def prepTweets(file, label):\n",
    "    \"\"\" preprocess the tweets by spliting each line into words, removing\n",
    "    useless symbols, tranforming all words into lower cases, removing \n",
    "    label word and stop words.\n",
    "    \n",
    "    inputs:\n",
    "    -------\n",
    "    - file: the text file containing data.\n",
    "    - label: the label of tweets inside this file.\n",
    "    \n",
    "    output:\n",
    "    -------\n",
    "    - tweets: the cleaned tweets dataset as an RDD. Each row contains the \n",
    "        cleaned words appeared in one tweet.\n",
    "    - n_tweet: the number of tweets in this dataset.\n",
    "    \"\"\"\n",
    "    # replace char ' with space\n",
    "    # split the line into list of words by space\n",
    "    # remove any characters are not alpha or number\n",
    "    # change characters to lower case\n",
    "    # remove label word\n",
    "    # remove empty string\n",
    "    # remove stop words\n",
    "    tweets = sc.textFile(file)\\\n",
    "        .map(lambda x: x.replace(\"'\", \" \"))\\\n",
    "        .map(lambda x: x.split(\" \"))\\\n",
    "        .map(lambda x: [re.sub(r'([^\\s\\w]|_)+', '', word) for word in x])\\\n",
    "        .map(lambda x: [word.lower() for word in x])\\\n",
    "        .map(lambda x: [word for word in x if word != label])\\\n",
    "        .map(lambda x: [word for word in x if word != ''])\\\n",
    "        .map(lambda x: [word for word in x if word not in STOPWORDS])\n",
    "    return tweets, tweets.count()\n",
    "\n",
    "resistTweets, n_resist = prepTweets(\"../data/resist.txt\", 'resist')\n",
    "magaTweets, n_maga     = prepTweets(\"../data/MAGA.txt\", 'maga')\n",
    "print(\"Number of resist tweets: %d\" %n_resist)\n",
    "print(\"Number of maga tweets: %d\" %n_maga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. feature extraction\n",
    "In this step, we mix 2 tweet datasets together, flatten the rdd so that each row is one word and then we count the frequency of each word, take the top 5000 ones as our feature words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rt',\n",
       " 'trump',\n",
       " 'funder',\n",
       " 'amp',\n",
       " 'realdonaldtrump',\n",
       " 'trumpresign',\n",
       " 'theresistance',\n",
       " 'president',\n",
       " 'cnn',\n",
       " 'potus')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets   = resistTweets.union(magaTweets)\n",
    "# flat the list so that each row has one word\n",
    "# add 1 count to each word\n",
    "# reduce them to get the word frequencies\n",
    "# take the first 5000 words\n",
    "featureCnt = tweets.flatMap(lambda x: x)\\\n",
    "    .map(lambda x: (x, 1))\\\n",
    "    .reduceByKey(lambda a, b: a+b)\\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\\\n",
    "    .take(5000)\n",
    "    \n",
    "features, counts = zip(*featureCnt)\n",
    "features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. transform dataset\n",
    "The original dataset is simply a list of words appeared in each tweet. Now we have the features, we will transform the dataset into a structured one with a fixed size of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_r = resistTweets.map(lambda x: [word in x for word in features])\n",
    "dataset_m = magaTweets.map(lambda x: [word in x for word in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. calculate \"informativeness\" for features\n",
    "Since all the feature values are binary(either 1 or 0), all of the features will have Binoulli distributions. We can easily get the probability of each feature conditioned on either label by summing up the values of each column and divide it by the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(word, word count in resist tweets, word count in maga tweets)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rt', 7317, 7669),\n",
       " ('trump', 1939, 2549),\n",
       " ('funder', 2422, 4),\n",
       " ('amp', 1103, 1063),\n",
       " ('realdonaldtrump', 780, 1376),\n",
       " ('trumpresign', 1271, 68),\n",
       " ('theresistance', 1358, 187),\n",
       " ('president', 204, 1188),\n",
       " ('cnn', 1009, 261),\n",
       " ('potus', 156, 1064),\n",
       " ('retweet', 1003, 190),\n",
       " ('speakerryan', 1041, 45),\n",
       " ('white', 658, 376),\n",
       " ('tonight', 847, 118),\n",
       " ('agree', 925, 70),\n",
       " ('america', 258, 609),\n",
       " ('able', 864, 14),\n",
       " ('question', 822, 8),\n",
       " ('trumpre', 754, 27),\n",
       " ('civil', 767, 13),\n",
       " ('town', 763, 9),\n",
       " ('gop', 416, 341),\n",
       " ('re', 444, 280),\n",
       " ('hall', 759, 7),\n",
       " ('unscripted', 757, 0),\n",
       " ('httpstco', 350, 394),\n",
       " ('nazis', 580, 157),\n",
       " ('people', 233, 397),\n",
       " ('charlottesville', 383, 242),\n",
       " ('left', 197, 422)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elementWiseAdd(list_1, list_2):\n",
    "    \"\"\" combine 2 lists together with element wise addition\"\"\"\n",
    "    return [a + b for a, b in zip(list_1, list_2)]\n",
    "\n",
    "wordCount_r = dataset_r.reduce(lambda a, b: elementWiseAdd(a, b))\n",
    "wordCount_m = dataset_m.reduce(lambda a, b: elementWiseAdd(a, b))\n",
    "wordCounts  = list(zip(features, wordCount_r, wordCount_m))\n",
    "\n",
    "print(\"(word, word count in resist tweets, word count in maga tweets)\")\n",
    "wordCounts[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate the informativeness of features defined by the biggest ratio of conditional probability:\n",
    "$$maxarg\\left( \\frac{p(true|label=resist)}{p(true|label=maga)}, \\frac{p(true|label=maga)}{p(true|label=resist)}\\right)$$\n",
    "\n",
    "So we need to divide the count by corresponding dataset size and get the ratio. But we observed that there is some words don't appear in one dataset at all. To avoid divide by zero error, we smooth the count by adding 1 to each of the count of each features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(word, logProb(word|label='resist'), logProb(word|label='maga'), informativeness)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('unscripted', (-2.7398181384577525, -9.29182818882245, 700.6511)),\n",
       " ('funder', (-1.5777398032835983, -7.68239027638835, 447.93605)),\n",
       " ('745pm', (-3.2860021110249527, -9.29182818882245, 405.78606)),\n",
       " ('assemble', (-3.299763796097634, -9.29182818882245, 400.24001)),\n",
       " ('tweetfest', (-3.3043934339963763, -9.29182818882245, 398.39133)),\n",
       " ('blackouttrump', (-3.376540096793555, -9.29182818882245, 370.66107)),\n",
       " ('dobbs', (-9.370501524100124, -3.571516412215039, 329.96451)),\n",
       " ('fbn7p', (-9.370501524100124, -3.5913846154317643, 323.47341)),\n",
       " ('kthopkins', (-9.370501524100124, -3.611655579805383, 316.9823)),\n",
       " ('six', (-9.370501524100124, -3.9165497811382854, 233.67979)),\n",
       " ('usaassociation', (-9.370501524100124, -4.008624460084462, 213.12462)),\n",
       " ('bfraser747', (-9.370501524100124, -4.018828630258704, 210.96092)),\n",
       " ('maga', (-3.981429794283623, -9.29182818882245, 202.43086)),\n",
       " ('draintheswamp', (-9.370501524100124, -4.093331157556625, 195.81501)),\n",
       " ('loudobbs', (-8.677354343540179, -3.4628825712122437, 183.91465)),\n",
       " ('raped', (-9.370501524100124, -4.167864209419192, 181.75094)),\n",
       " ('ahistoric', (-9.370501524100124, -4.18588271492187, 178.50539)),\n",
       " ('1942bs', (-9.370501524100124, -4.18588271492187, 178.50539)),\n",
       " ('terrorized', (-9.370501524100124, -4.18588271492187, 178.50539)),\n",
       " ('panther', (-9.370501524100124, -4.18588271492187, 178.50539)),\n",
       " ('ironstache', (-4.123477451939638, -9.29182818882245, 175.62495)),\n",
       " ('harlan', (-9.370501524100124, -4.235582383474143, 169.85059)),\n",
       " ('biracial', (-9.370501524100124, -4.308221567114114, 157.95023)),\n",
       " ('trumpputin', (-4.234703087049862, -9.29182818882245, 157.13811)),\n",
       " ('trumpisatraitor', (-4.234703087049862, -9.29182818882245, 157.13811)),\n",
       " ('exgirlfriend', (-9.370501524100124, -4.343068298444282, 152.54097)),\n",
       " ('breaks', (-9.370501524100124, -4.357354255691759, 150.37727)),\n",
       " ('kara', (-9.370501524100124, -4.364574503665246, 149.29542)),\n",
       " ('resistÃªncia', (-4.289097159115661, -9.29182818882245, 148.81903)),\n",
       " ('pjnet', (-9.370501524100124, -4.379173303086398, 147.13172))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcInf(cps):\n",
    "    \"\"\" takes in the conditional probablities of one feature and calculate\n",
    "    the informativeness of this feature.\n",
    "    \"\"\"\n",
    "    return round(max(cps[0]/cps[1], cps[1]/cps[0]), 5)\n",
    "\n",
    "# parallelize the word counts into rdd\n",
    "# orgaize the rows as (word, (count_resist, count_maga))\n",
    "# add one to each count to avoid zero divide error\n",
    "# calculate the conditioning probabilities \n",
    "# calculate the informativeness\n",
    "infs = sc.parallelize(wordCounts)\\\n",
    "    .map(lambda x: (x[0], (x[1], x[2])))\\\n",
    "    .mapValues(lambda x: (x[0]+1, x[1]+1))\\\n",
    "    .mapValues(lambda x: (x[0]/n_resist, x[1]/n_maga))\\\n",
    "    .mapValues(lambda x: (log(x[0]), log(x[1]), calcInf(x)))\n",
    "    \n",
    "# sort the features words by informativeness and collect them into master machine.\n",
    "informativeness = infs\\\n",
    "    .sortBy(lambda x: x[1][2], ascending=False)\\\n",
    "    .collect()\n",
    "    \n",
    "infWords, _ = zip(*informativeness)\n",
    "print(\"(word, logProb(word|label='resist'), logProb(word|label='maga'), informativeness)\")\n",
    "informativeness[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
